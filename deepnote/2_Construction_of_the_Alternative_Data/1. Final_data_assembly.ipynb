{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "This notebook gathers the data collected from our buffers and alternative buffers contructed in the notebooks `Construct-Buffers-v17`, `Construct-Buffers-v16`, `Construct-AltBuffers-v17.ipynb`, and `Construct-AltBuffers-v16` and combines it with Dicken's own dataset.\n\nNote that each of the above-mentioned notebooks require the Ethnologe to run which is protected under copy rights. Therefore, these notebooks can't be run here on Deepnote. However, you can see the outputs of each cell. If you want to understand how the data was constructed please review the notebooks.",
      "metadata": {
        "cell_id": "946e68ffabb64e178e82c4f3764e102e",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "45f2f8a3cabc4a31abdc6c2bbae7fc24"
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nfrom scipy.stats import zscore\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport sys, os, time\npd.set_option('display.width', 140)\n\n# Plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(11.7,8.27)})\nsns.set_context(\"talk\")\n\nfrom IPython.display import display, HTML, Image\n\npathdata = '/work/Replication_Dickens_2022/data/'",
      "metadata": {
        "source_hash": null,
        "execution_start": 1694029597576,
        "execution_millis": 6271,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "989300fe877b4edea6f28b11811fc5d0",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "IPyStata is loaded in batch mode.\n"
        }
      ],
      "execution_count": null,
      "block_group": "31347a74baa24b2382351c613213590a"
    },
    {
      "cell_type": "markdown",
      "source": "## Preparing the Final Dataset",
      "metadata": {
        "cell_id": "5fb8354adde3423a9694536cc4c883a5",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "8398db433a97449dacab7bfc28922190"
    },
    {
      "cell_type": "code",
      "source": "# Load all datasets\ndfor = pd.read_stata(pathdata + 'EJ_Dickens_Border_100km.dta')\ndfor_v16 = pd.read_stata(pathdata + 'Dickens_OrBuf_v16_stats.dta')\ndfor_v17 = pd.read_stata(pathdata + 'Dickens_OrBuf_v17_stats.dta')\ndfor_v16_alt = pd.read_stata(pathdata + 'Dickens_AltBuf_v16_absdif.dta')\ndfor_v17_alt = pd.read_stata(pathdata + 'Dickens_AltBuf_v17_absdif.dta')\n\n# Redefine some variables to be closer to what Dickens did with buffers defined by Dickens\nfor df in [dfor_v16,dfor_v17]:\n    df['csi_change_sd_oj'] = (df.post1500AverageCaloriesstd - df.pre1500AverageCaloriesstd)/1000\n    df['csi_sd_oj'] = (df.pre1500AverageCaloriesstd)/1000\n    df['csi_change_oj'] = (df.post1500AverageCaloriesmean - df.pre1500AverageCaloriesmean)/1000\n    df['csi_oj'] = (df.pre1500AverageCaloriesmean)/1000\n\n# Redefine some variables to be closer to what Dickens did with the alternative buffers\nfor df in [dfor_v16_alt,dfor_v17_alt]:\n    df['csi_change_alt'] = (df.post1500AverageCaloriesmean - df.pre1500AverageCaloriesmean)/1000\n    df['csi_alt'] = (df.pre1500AverageCaloriesmean)/1000",
      "metadata": {
        "source_hash": null,
        "execution_start": 1694029603852,
        "execution_millis": 9354,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "9e1ae4e15ec9459e934da5afaa52824f",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_55/3998832166.py:2: UnicodeWarning: \nOne or more strings in the dta file could not be decoded using utf-8, and\nso the fallback encoding of latin-1 is being used.  This can happen when a file\nhas been incorrectly encoded by Stata or some other software. You should verify\nthe string values returned are correct.\n  dfor = pd.read_stata(pathdata + 'EJ_Dickens_Border_100km.dta')\n/tmp/ipykernel_55/3998832166.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df['csi_change_sd_oj'] = (df.post1500AverageCaloriesstd - df.pre1500AverageCaloriesstd)/1000\n/tmp/ipykernel_55/3998832166.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df['csi_sd_oj'] = (df.pre1500AverageCaloriesstd)/1000\n/tmp/ipykernel_55/3998832166.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df['csi_change_oj'] = (df.post1500AverageCaloriesmean - df.pre1500AverageCaloriesmean)/1000\n/tmp/ipykernel_55/3998832166.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df['csi_oj'] = (df.pre1500AverageCaloriesmean)/1000\n/tmp/ipykernel_55/3998832166.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df['csi_change_sd_oj'] = (df.post1500AverageCaloriesstd - df.pre1500AverageCaloriesstd)/1000\n/tmp/ipykernel_55/3998832166.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df['csi_sd_oj'] = (df.pre1500AverageCaloriesstd)/1000\n/tmp/ipykernel_55/3998832166.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df['csi_change_oj'] = (df.post1500AverageCaloriesmean - df.pre1500AverageCaloriesmean)/1000\n/tmp/ipykernel_55/3998832166.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df['csi_oj'] = (df.pre1500AverageCaloriesmean)/1000\n/tmp/ipykernel_55/3998832166.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df['csi_change_alt'] = (df.post1500AverageCaloriesmean - df.pre1500AverageCaloriesmean)/1000\n/tmp/ipykernel_55/3998832166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df['csi_alt'] = (df.pre1500AverageCaloriesmean)/1000\n/tmp/ipykernel_55/3998832166.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df['csi_change_alt'] = (df.post1500AverageCaloriesmean - df.pre1500AverageCaloriesmean)/1000\n/tmp/ipykernel_55/3998832166.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df['csi_alt'] = (df.pre1500AverageCaloriesmean)/1000\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null,
      "block_group": "b8266cf3fc6a456f8845061a8553590a"
    },
    {
      "cell_type": "code",
      "source": "# We want to make sure that we have the same as buffers zones Dicken, \n# so we will first restrict our sample to those that have information about lingDist\ndfor = dfor[dfor.lingDist.isna() == False]\n\n# Merge the data set for v16\ndfor16 = dfor.merge(dfor_v16, how = 'left', on = 'identifier')\ndfor16 = dfor16.merge(dfor_v16_alt, how = 'left', on = 'identifier')\n\n# Merge the data set for v16\ndfor17 = dfor.merge(dfor_v17, how = 'left', on = 'identifier')\ndfor17 = dfor17.merge(dfor_v17_alt, how = 'left', on = 'identifier')",
      "metadata": {
        "source_hash": null,
        "execution_start": 1694029613075,
        "execution_millis": 1166,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "f65fe7114be24461874536c6a69006b3",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "ec6aa703595446eaa3fc90526558d020"
    },
    {
      "cell_type": "code",
      "source": "# There is an error in the data v17 where a buffer is reapted 3 times\ncategory_counts = dfor17.identifier.value_counts()\nsingle_obs_categories = category_counts[category_counts != 1].index.tolist()\nsingle_obs_categories\n\n# To fix this we are going to drop these buffers\ndfor17 = dfor17[~dfor17.identifier.isin(single_obs_categories)]\n\n# We will also further restrict the sample to those that we have data on csi_alt\ndfor17 = dfor17[dfor17.csi_alt.isna() == False]\n\n# Now we check the number of observations.\nprint(sum(dfor16.csi_alt.isna() == False))\nprint(sum(dfor17.csi.isna() == False))",
      "metadata": {
        "source_hash": null,
        "execution_start": 1694029614252,
        "execution_millis": 132,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "1c34c04576234a9db603f9fa3cc6a196",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "8426\n7582\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null,
      "block_group": "dc1f8ca6698e41eea3d216a500327d18"
    },
    {
      "cell_type": "code",
      "source": "# To get the same number of observations in the regressions we also need to identify family1 and family2 singletons\ncategory_counts = dfor16.loc[dfor16.lingDist.isna()==False].groupby('family1').identifier.count()\nsingletop_fam1 = category_counts[category_counts == 1].index.tolist()\n\ncategory_counts = dfor16.loc[dfor16.lingDist.isna()==False].groupby('family2').identifier.count()\nsingletop_fam2 = category_counts[category_counts == 1].index.tolist()\n\ndfor16 = dfor16[(~dfor16.family1.isin(singletop_fam1)) & (~dfor16.family2.isin(singletop_fam2))]\nprint(sum(dfor16.csi_alt.isna() == False))\ndfor17 = dfor17[(~dfor17.family1.isin(singletop_fam1)) & (~dfor17.family2.isin(singletop_fam2))]\nprint(sum(dfor17.csi_alt.isna() == False))",
      "metadata": {
        "source_hash": null,
        "execution_start": 1694029614365,
        "execution_millis": 124,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "dc7088d361bd47a6966ba048f8e94b04",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "8402\n7564\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null,
      "block_group": "1edc08bfd51b43a29358c31bcea9d2ad"
    },
    {
      "cell_type": "code",
      "source": "# For specification (6) in Table 1 he also drops the singletons when they are in the same country.\ndfor_same_country = dfor16[dfor16.samecountry == 1]\nprint(dfor_same_country.shape)\ncategory_counts = dfor_same_country.groupby('family1').identifier.count()\nsingletop_fam1 = category_counts[category_counts == 1].index.tolist()\n\ncategory_counts = dfor_same_country.groupby('family2').identifier.count()\nsingletop_fam2 = category_counts[category_counts == 1].index.tolist()\n\ncategory_counts = dfor_same_country.groupby('ccode1').identifier.count()\nsingletop_ccode = category_counts[category_counts == 1].index.tolist()\n\ndfor_same_country = dfor_same_country[(~dfor_same_country.family1.isin(singletop_fam1)) & (~dfor_same_country.family2.isin(singletop_fam2)) & (~dfor_same_country.ccode1.isin(singletop_ccode))]\nprint(sum(dfor_same_country.csi_alt.isna() == False))",
      "metadata": {
        "source_hash": null,
        "execution_start": 1694029614520,
        "execution_millis": 16,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "70531afb192a4ffdbcbc53fab7146e36",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "(7312, 409)\n7291\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null,
      "block_group": "3247ea5382fc4af99ca3ba9672763849"
    },
    {
      "cell_type": "code",
      "source": "# Here is the code to export the final data used for the analysis\n\n# Get the variables used in the analysis and order them with the csi variables 1st\nvar_to_keep = dfor.columns.tolist()\nvar_to_keep.remove(\"csi\")\nvar_to_keep.remove(\"csi_sd\")\nvar_to_keep.remove(\"csi_change\")\nvar_to_keep.remove(\"csi_change_sd\")\n\nvar_to_keep = ['csi','csi_sd','csi_change','csi_change_sd','csi_oj','csi_change_oj','csi_sd_oj','csi_change_sd_oj','csi_alt','csi_change_alt'] + var_to_keep\n\n# Export both datasets to .dta format\ndfor16[var_to_keep].to_stata(pathdata + 'Dickens_rep_v16.dta', version=117)\ndfor17[var_to_keep].to_stata(pathdata + 'Dickens_rep_v17.dta', version=117)",
      "metadata": {
        "source_hash": null,
        "execution_start": 1694029614564,
        "execution_millis": 1123,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "262e2acb08f1484bb153aa0be418cccb",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "7ebd2b8af26443ecb2f09437ea2523d5"
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=4a945f27-2c4c-4244-8f3b-ab6dff812a2f' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote": {},
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "nbconvert_exporter": "python"
    },
    "deepnote_notebook_id": "2ba020fd3af94318acc227a871cca395",
    "deepnote_execution_queue": []
  }
}