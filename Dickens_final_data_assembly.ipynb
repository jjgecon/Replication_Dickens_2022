{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e95c3b9a-a9f4-4f70-8032-80064d5dcb32",
   "metadata": {},
   "source": [
    "This notebook gathers the data collected from our buffers and alternative buffers contructed in the notebooks `Replication-Construct-Buffers-Touch-v17`, `Replication-Construct-Buffers-Touch-v17`, `Replication-Construct-AltBuffers-Touch-v16`, and `Replication-Construct-AltBuffers-Touch-v16` and combines it with Dicken's own dataset.\n",
    "\n",
    "Note that each of the mentioned notebooks require the Ethnologe to run which is protected under copy rights. Therefore, these notebooks can't be run here on Deepnote. However, you can see the outputs of each cell. If you want to understand how the data was constructed please review the notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdae7b91-b400-402f-bdcd-5ac06844fca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPyStata is loaded in batch mode.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import sys, os, time\n",
    "pd.set_option('display.width', 140)\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "from IPython.display import display, HTML, Image\n",
    "\n",
    "pathdata = '/work/Replication_Dickens_2022/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126aa9ea-d742-4359-91d5-f2cf0ac3505e",
   "metadata": {},
   "source": [
    "## Preparing the Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cf8dbe4-7d3c-4f50-a54b-f86b2dff9cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3058165/3472773383.py:2: UnicodeWarning: \n",
      "One or more strings in the dta file could not be decoded using utf-8, and\n",
      "so the fallback encoding of latin-1 is being used.  This can happen when a file\n",
      "has been incorrectly encoded by Stata or some other software. You should verify\n",
      "the string values returned are correct.\n",
      "  dfor = pd.read_stata(pathdickens + 'Data/EJ_Dickens_Border_100km.dta')\n",
      "/tmp/ipykernel_3058165/3472773383.py:2: UnicodeWarning: \n",
      "One or more strings in the dta file could not be decoded using utf-8, and\n",
      "so the fallback encoding of latin-1 is being used.  This can happen when a file\n",
      "has been incorrectly encoded by Stata or some other software. You should verify\n",
      "the string values returned are correct.\n",
      "  dfor = pd.read_stata(pathdickens + 'Data/EJ_Dickens_Border_100km.dta')\n",
      "/tmp/ipykernel_3058165/3472773383.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['csi_change_sd_oj'] = (df.post1500AverageCaloriesstd - df.pre1500AverageCaloriesstd)/1000\n",
      "/tmp/ipykernel_3058165/3472773383.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['csi_sd_oj'] = (df.pre1500AverageCaloriesstd)/1000\n",
      "/tmp/ipykernel_3058165/3472773383.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['csi_change_oj'] = (df.post1500AverageCaloriesmean - df.pre1500AverageCaloriesmean)/1000\n",
      "/tmp/ipykernel_3058165/3472773383.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['csi_oj'] = (df.pre1500AverageCaloriesmean)/1000\n",
      "/tmp/ipykernel_3058165/3472773383.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['csi_change_sd_oj'] = (df.post1500AverageCaloriesstd - df.pre1500AverageCaloriesstd)/1000\n",
      "/tmp/ipykernel_3058165/3472773383.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['csi_sd_oj'] = (df.pre1500AverageCaloriesstd)/1000\n",
      "/tmp/ipykernel_3058165/3472773383.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['csi_change_oj'] = (df.post1500AverageCaloriesmean - df.pre1500AverageCaloriesmean)/1000\n",
      "/tmp/ipykernel_3058165/3472773383.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['csi_oj'] = (df.pre1500AverageCaloriesmean)/1000\n",
      "/tmp/ipykernel_3058165/3472773383.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['csi_change_alt'] = (df.post1500AverageCaloriesmean - df.pre1500AverageCaloriesmean)/1000\n",
      "/tmp/ipykernel_3058165/3472773383.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['csi_alt'] = (df.pre1500AverageCaloriesmean)/1000\n",
      "/tmp/ipykernel_3058165/3472773383.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['csi_change_alt'] = (df.post1500AverageCaloriesmean - df.pre1500AverageCaloriesmean)/1000\n",
      "/tmp/ipykernel_3058165/3472773383.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['csi_alt'] = (df.pre1500AverageCaloriesmean)/1000\n"
     ]
    }
   ],
   "source": [
    "# Load all datasets\n",
    "dfor = pd.read_stata(pathdata + 'EJ_Dickens_Border_100km.dta')\n",
    "dfor_v16 = pd.read_stata(pathdata + 'Dickens_OrBuf_v16_stats.dta')\n",
    "dfor_v17 = pd.read_stata(pathdata + 'Dickens_OrBuf_v17_stats.dta')\n",
    "dfor_v16_alt = pd.read_stata(pathdata + 'Dickens_AltBuf_v16_absdif.dta')\n",
    "dfor_v17_alt = pd.read_stata(pathdata + 'Dickens_AltBuf_v17_absdif.dta')\n",
    "\n",
    "# Redefine some variables to be closer to what Dickens did with buffers defined by Dickens\n",
    "for df in [dfor_v16,dfor_v17]:\n",
    "    df['csi_change_sd_oj'] = (df.post1500AverageCaloriesstd - df.pre1500AverageCaloriesstd)/1000\n",
    "    df['csi_sd_oj'] = (df.pre1500AverageCaloriesstd)/1000\n",
    "    df['csi_change_oj'] = (df.post1500AverageCaloriesmean - df.pre1500AverageCaloriesmean)/1000\n",
    "    df['csi_oj'] = (df.pre1500AverageCaloriesmean)/1000\n",
    "\n",
    "# Redefine some variables to be closer to what Dickens did with the alternative buffers\n",
    "for df in [dfor_v16_alt,dfor_v17_alt]:\n",
    "    df['csi_change_alt'] = (df.post1500AverageCaloriesmean - df.pre1500AverageCaloriesmean)/1000\n",
    "    df['csi_alt'] = (df.pre1500AverageCaloriesmean)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52769f02-1593-44be-99d3-7f3b1b965bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to make sure that we have the same as buffers zones Dicken, \n",
    "# so we will first restrict our sample to those that have information about lingDist\n",
    "dfor = dfor[dfor.lingDist.isna() == False]\n",
    "\n",
    "# Merge the data set for v16\n",
    "dfor16 = dfor.merge(dfor_v16, how = 'left', on = 'identifier')\n",
    "dfor16 = dfor16.merge(dfor_v16_alt, how = 'left', on = 'identifier')\n",
    "\n",
    "# Merge the data set for v16\n",
    "dfor17 = dfor.merge(dfor_v17, how = 'left', on = 'identifier')\n",
    "dfor17 = dfor17.merge(dfor_v17_alt, how = 'left', on = 'identifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78436f1f-ce6b-41e9-ada3-117bddccbc4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALN-SRB-SRP-SRB', 'ALN-SRB-MKD-MKD', 'ALN-SRB-SRP-MKD']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There is an error in the data v17 where a buffer is reapted 3 times\n",
    "category_counts = dfor17.identifier.value_counts()\n",
    "single_obs_categories = category_counts[category_counts != 1].index.tolist()\n",
    "single_obs_categories\n",
    "\n",
    "# To fix this we are going to drop these buffers\n",
    "dfor17 = dfor17[~dfor17.identifier.isin(single_obs_categories)]\n",
    "\n",
    "# We will also further restrict the sample to those that we have data on csi_alt\n",
    "dfor17 = dfor17[dfor17.csi_alt.isna() == False]\n",
    "\n",
    "# Now we check the number of observations.\n",
    "print(sum(dfor16.csi_alt.isna() == False))\n",
    "print(sum(dfor17.csi.isna() == False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae6d04d2-46f5-4159-8cbc-e0add2c5afee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8402\n",
      "7564\n"
     ]
    }
   ],
   "source": [
    "# To get the same number of observations in the regressions we also need to identify family1 and family2 singletons\n",
    "category_counts = dfor16.loc[dfor16.lingDist.isna()==False].groupby('family1').identifier.count()\n",
    "singletop_fam1 = category_counts[category_counts == 1].index.tolist()\n",
    "\n",
    "category_counts = dfor16.loc[dfor16.lingDist.isna()==False].groupby('family2').identifier.count()\n",
    "singletop_fam2 = category_counts[category_counts == 1].index.tolist()\n",
    "\n",
    "dfor16 = dfor16[(~dfor16.family1.isin(singletop_fam1)) & (~dfor16.family2.isin(singletop_fam2))]\n",
    "print(sum(dfor16.csi_alt.isna() == False))\n",
    "dfor17 = dfor17[(~dfor17.family1.isin(singletop_fam1)) & (~dfor17.family2.isin(singletop_fam2))]\n",
    "print(sum(dfor17.csi_alt.isna() == False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42979784-7600-46a8-978a-10ad7ded61c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7312, 409)\n",
      "7291\n"
     ]
    }
   ],
   "source": [
    "# For specification (6) in Table 1 he also drops the singletons when they are in the same country.\n",
    "dfor_same_country = dfor16[dfor16.samecountry == 1]\n",
    "print(dfor_same_country.shape)\n",
    "category_counts = dfor_same_country.groupby('family1').identifier.count()\n",
    "singletop_fam1 = category_counts[category_counts == 1].index.tolist()\n",
    "\n",
    "category_counts = dfor_same_country.groupby('family2').identifier.count()\n",
    "singletop_fam2 = category_counts[category_counts == 1].index.tolist()\n",
    "\n",
    "category_counts = dfor_same_country.groupby('ccode1').identifier.count()\n",
    "singletop_ccode = category_counts[category_counts == 1].index.tolist()\n",
    "\n",
    "dfor_same_country = dfor_same_country[(~dfor_same_country.family1.isin(singletop_fam1)) & (~dfor_same_country.family2.isin(singletop_fam2)) & (~dfor_same_country.ccode1.isin(singletop_ccode))]\n",
    "print(sum(dfor_same_country.csi_alt.isna() == False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef6a72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the code to export the final data used for the analysis\n",
    "\n",
    "# Get the variables used in the analysis and order them with the csi variables 1st\n",
    "var_to_keep = dfor.columns.tolist()\n",
    "var_to_keep.remove(\"csi\")\n",
    "var_to_keep.remove(\"csi_sd\")\n",
    "var_to_keep.remove(\"csi_change\")\n",
    "var_to_keep.remove(\"csi_change_sd\")\n",
    "\n",
    "var_to_keep = ['csi','csi_sd','csi_change','csi_change_sd','csi_oj','csi_change_oj','csi_sd_oj','csi_change_sd_oj','csi_alt','csi_change_alt'] + var_to_keep\n",
    "\n",
    "# Export both datasets to .dta format\n",
    "dfor16[var_to_keep].to_stata(pathdata + 'Dickens_rep_v16.dta', version=117)\n",
    "dfor17[var_to_keep].to_stata(pathdata + 'Dickens_rep_v17.dta', version=117)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
